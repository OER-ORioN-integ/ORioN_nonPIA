{
  "name" : "orion_prd_etl-job_normalization",
  "jobMode" : "VISUAL",
  "description" : "",
  "role" : "arn:aws:iam::142980225941:role/orion_prd_glue-role",
  "executionProperty" : {
    "maxConcurrentRuns" : 1
  },
  "command" : {
    "name" : "glueetl",
    "scriptLocation" : "s3://orion-prd-etljob/scripts/orion_prd_etl-job_normalization.py",
    "pythonVersion" : "3"
  },
  "defaultArguments" : {
    "--enable-metrics" : "true",
    "--spark-event-logs-path" : "s3://aws-glue-assets-142980225941-ap-northeast-1/sparkHistoryLogs/",
    "--enable-job-insights" : "false",
    "--enable-glue-datacatalog" : "true",
    "--EXECUTE_TIMESTAMP_ISO_STR" : "2024-11-01T12:23:45.123Z",
    "--SOURCE_SYSTEM" : "ソースシステム名",
    "--job-bookmark-option" : "job-bookmark-disable",
    "--DATA_NAME" : "物理テーブル名",
    "--job-language" : "python",
    "--TempDir" : "s3://orion-prd-etljob/tmp/"
  },
  "maxRetries" : 0,
  "allocatedCapacity" : 2,
  "timeout" : 120,
  "maxCapacity" : 2.0,
  "glueVersion" : "4.0",
  "numberOfWorkers" : 2,
  "workerType" : "G.1X",
  "executionClass" : "STANDARD",
  "codeGenConfigurationNodes" : "{\"node-1731895654366\":{\"SelectFromCollection\":{\"Index\":0,\"Inputs\":[\"node-1731895643834\"],\"Name\":\"【加工時必須】加工後処理(定義した日付フォーマットに変換)\"}},\"node-1733131840543\":{\"CustomCode\":{\"ClassName\":\"MyTransformTimestampFormat\",\"Code\":\"from pyspark.sql.functions import col, lit, regexp_replace, when\\n\\n### 作成時の修正箇所ここから ###\\n# 入力項目名と出力項目名の設定\\n入力項目名リスト = [\\\"入力項目名１\\\", \\\"入力項目名２\\\"]\\n出力項目名リスト = [\\\"出力項目名１\\\", \\\"出力項目名２\\\"]\\n### 作成時の修正箇所ここまで ###\\n\\n# 入力フォーマット\\nINPUT_FORMAT = r\\\"^\\\\d{4}-\\\\d{2}-\\\\d{2}\\\"\\n\\n# 最小値の設定\\nMIN_YMD = lit(\\\"1900-01-01\\\")\\nMIN_TIMESTAMP_FORMAT = None\\n\\n# 条件文：\\n#   1)Nullならそのまま、\\n#   2)頭の10文字(年月日部分)を取得し、YYYY-MM-DDのフォーマットかチェックし、\\n#       2-1) 年月日部分がMIN_YMD より小さいなら MIN_YMD_FORMAT に置換\\n#       2-2)それ以外なら, 19文字目までを取得(ミリ秒以下を除外)し、\\\"-\\\"を\\\"/\\\"に置換\\n#   3)timestamp文字列でないないら、Noneを返す\\ndef mk_cond(col_name: str):\\n    return (\\n        when(col(col_name).isNull(), None)\\n        .when(col(col_name).substr(1, 10).rlike(INPUT_FORMAT),\\n            when(col(col_name).substr(1, 10) < MIN_YMD, MIN_TIMESTAMP_FORMAT)\\n            .otherwise(regexp_replace(col(col_name).substr(1, 19), \\\"-\\\", \\\"/\\\")))\\n        .otherwise(None)\\n    )\\n\\nconds_dict = {出力項目名: mk_cond(入力項目名) for 入力項目名, 出力項目名 in zip(入力項目名リスト, 出力項目名リスト)}\\n\\n# DataFrameの取得\\ndf = dfc.select(list(dfc.keys())[0]).toDF()\\n\\n# 値のマスキング実施\\ndf = df.withColumns(conds_dict)\\n\\n# DynamicFrameに変換して返す\\noutput_dyf = DynamicFrame.fromDF(df, glueContext, \\\"output\\\")\\nreturn DynamicFrameCollection({\\\"CustomTransform0\\\": output_dyf}, glueContext)\",\"Inputs\":[\"node-1727678518200\"],\"Name\":\"【加工】定義した日時フォーマットへ変換\"}},\"node-1727678461781\":{\"CustomCode\":{\"ClassName\":\"MyTransformConvertToDecidedTimeFormat\",\"Code\":\"from pyspark.sql.functions import udf\\nfrom pyspark.sql.types import StringType\\nfrom datetime import datetime\\n\\n### 作成時の修正箇所ここから ###\\n# 入力項目名と出力項目名の設定\\n入力項目名リスト = [\\\"入力項目名１\\\", \\\"入力項目名２\\\"]\\n出力項目名リスト = [\\\"出力項目名１\\\", \\\"出力項目名２\\\"]\\n### 作成時の修正箇所ここまで ###\\n\\n# 入力フォーマット\\nINPUT_FORMAT = \\\"%H%M\\\"\\n# 出力フォーマット\\nOUTPUT_FORMAT = \\\"%H:%M:00\\\"\\n\\n# フォーマット変換関数の定義\\ndef convert_datetime(datetime_str):\\n    if not datetime_str or len(datetime_str.strip()) == 0:\\n        # 空の文字列の場合はNoneを返すなど、適切な処理を行う\\n        return None\\n    else:\\n        try:\\n            datetime_obj = datetime.strptime(datetime_str, INPUT_FORMAT)\\n            return datetime_obj.strftime(OUTPUT_FORMAT)\\n        except ValueError:\\n            # 不正な値の場合Noneを返す\\n            return None\\n\\n# UDFの定義\\nconvert_datetime_udf = udf(convert_datetime, StringType())\\n\\n# データフレームの取得\\ndf = dfc.select(list(dfc.keys())[0]).toDF()\\n\\n# UDFを適用して新しいカラムを追加\\nfor 入力項目名, 出力項目名 in zip(入力項目名リスト, 出力項目名リスト):\\n    df = df.withColumn(出力項目名, convert_datetime_udf(df[入力項目名]))\\n\\n# DynamicFrameに変換\\noutput_dyf = DynamicFrame.fromDF(df, glueContext, \\\"output\\\")\\nreturn DynamicFrameCollection({\\\"CustomTransform0\\\": output_dyf}, glueContext)\\n\",\"Inputs\":[\"node-1731895654366\"],\"Name\":\"【加工】定義した時間フォーマットへ変換\",\"OutputSchemas\":[{\"Columns\":[{\"Name\":\"work_dt\",\"Type\":\"string\"},{\"Name\":\"odakyu_cust_num\",\"Type\":\"string\"},{\"Name\":\"agt_num\",\"Type\":\"string\"},{\"Name\":\"agt_idtfer_num\",\"Type\":\"string\"},{\"Name\":\"rpstve_cmpy_cd\",\"Type\":\"string\"},{\"Name\":\"cmpy_cd\",\"Type\":\"string\"},{\"Name\":\"office_cd\",\"Type\":\"string\"},{\"Name\":\"store_cd\",\"Type\":\"string\"},{\"Name\":\"trnl_num\",\"Type\":\"string\"},{\"Name\":\"trd_num\",\"Type\":\"string\"},{\"Name\":\"trd_knd\",\"Type\":\"string\"},{\"Name\":\"grt_div\",\"Type\":\"string\"},{\"Name\":\"trd_total_amtmny\",\"Type\":\"decimal(10,2)\"},{\"Name\":\"point_sub_amtmny\",\"Type\":\"decimal(10,2)\"},{\"Name\":\"trd_grt_point\",\"Type\":\"decimal(10,2)\"},{\"Name\":\"trd_gvbk_point\",\"Type\":\"decimal(10,2)\"},{\"Name\":\"trnl_dt\",\"Type\":\"string\"},{\"Name\":\"trnl_tm\",\"Type\":\"string\"},{\"Name\":\"dwh_reg_datetm\",\"Type\":\"timestamp\"},{\"Name\":\"orion_process_datetm\",\"Type\":\"string\"}]}]}},\"node-1727678518200\":{\"SelectFromCollection\":{\"Index\":0,\"Inputs\":[\"node-1727678461781\"],\"Name\":\"【加工時必須】加工後処理(定義した時間フォーマットへ変換)\"}},\"node-1731895939959\":{\"SelectFromCollection\":{\"Index\":0,\"Inputs\":[\"node-1731895901595\"],\"Name\":\"【手動アップロード必須】加工後処理(データ統合基盤処理日時付加)\"}},\"node-1727674889392\":{\"S3CsvSource\":{\"AdditionalOptions\":{\"EnableSamplePath\":true,\"SamplePath\":\"s3://orion-prd-work/(DMP,ONE,OCTPASS,MANUAL)/(DataName)/schema/(拡張子付ファイル名)\"},\"Escaper\":\"\\\\\",\"Exclusions\":[],\"Name\":\"【必須】連携された匿名化済みのデータ\",\"OptimizePerformance\":false,\"OutputSchemas\":[{\"Columns\":[]}],\"Paths\":[\"s3://orion-prd-work/(DMP,ONE,OCTPASS,MANUAL)/(DataName)/raw_tmp/\"],\"QuoteChar\":\"quote\",\"Separator\":\"comma\",\"WithHeader\":true}},\"node-1731895643834\":{\"CustomCode\":{\"ClassName\":\"MyTransformDateFormat\",\"Code\":\"from pyspark.sql.functions import col, lit, regexp_replace, when\\n\\n### 作成時の修正箇所ここから ###\\n# 入力項目名と出力項目名の設定\\n入力項目名リスト = [\\\"入力項目名１\\\", \\\"入力項目名２\\\"]\\n出力項目名リスト = [\\\"出力項目名１\\\", \\\"出力項目名２\\\"]\\n### 作成時の修正箇所ここまで ###\\n\\n# 入力フォーマット\\nINPUT_FORMAT = r\\\"^\\\\d{4}-\\\\d{2}-\\\\d{2}\\\"\\n\\n# 最小値の設定\\nMIN_YMD = lit(\\\"1900-01-01\\\")\\nMIN_YMD_FORMAT = None\\n\\n# 条件文：\\n#   1)Nullならそのまま\\n#   2)Date文字列で、\\n#       2-1) MIN_YMD より小さいなら MIN_YMD_FORMAT に置換\\n#       2-2)それ以外なら\\\"-\\\"を\\\"/\\\"に置換\\n#   3)Date文字列でないないら、Noneを返す\\ndef mk_cond(col_name: str):\\n    return (\\n        when(col(col_name).isNull(), None)\\n        .when(col(col_name).substr(1, 10).rlike(INPUT_FORMAT),\\n            when(col(col_name).substr(1, 10) < MIN_YMD, MIN_YMD_FORMAT)\\n            .otherwise(regexp_replace(col(col_name).substr(1, 10), \\\"-\\\", \\\"/\\\")))\\n        .otherwise(None)\\n    )\\n\\nconds_dict = {出力項目名: mk_cond(入力項目名) for 入力項目名, 出力項目名 in zip(入力項目名リスト, 出力項目名リスト)}\\n\\n# DataFrameの取得\\ndf = dfc.select(list(dfc.keys())[0]).toDF()\\n\\n# 値のマスキング実施\\ndf = df.withColumns(conds_dict)\\n\\n# DynamicFrameに変換して返す\\noutput_dyf = DynamicFrame.fromDF(df, glueContext, \\\"output\\\")\\nreturn DynamicFrameCollection({\\\"CustomTransform0\\\": output_dyf}, glueContext)\",\"Inputs\":[\"node-1727676004506\"],\"Name\":\"【加工】定義した日付フォーマットに変換\"}},\"node-1727676004506\":{\"SelectFromCollection\":{\"Index\":0,\"Inputs\":[\"node-1727675945895\"],\"Name\":\"【加工時必須】加工後処理(８桁文字列を日付フォーマットへ)\"}},\"node-1727675890558\":{\"ApplyMapping\":{\"Inputs\":[\"node-1727674889392\"],\"Mapping\":[],\"Name\":\"【必須】データ型を変換\"}},\"node-1728609073799\":{\"CustomCode\":{\"ClassName\":\"MyTransformOutputOneFileParquet\",\"Code\":\"from datetime import datetime, timedelta, timezone\\nimport boto3\\nfrom awsglue.utils import getResolvedOptions\\nfrom pyspark.sql import DataFrame\\n\\n### 作成時の修正箇所ここから ###\\n#  本ノードは作成時のパラメータ設定不要です。このままお使いください。\\n### 作成時の修正箇所ここまで ###\\n\\nprint(f\\\"{datetime.strftime(datetime.now(), '%Y/%m/%dT%H:%M:%S')} INFO (MyTransformOutputOneFileParquet) start\\\")\\n\\n# Glueジョブ引数の取得\\nglue_params = getResolvedOptions(sys.argv, [\\\"SOURCE_SYSTEM\\\", \\\"DATA_NAME\\\", \\\"EXECUTE_TIMESTAMP_ISO_STR\\\"])\\nprint(f\\\"{datetime.strftime(datetime.now(), '%Y/%m/%dT%H:%M:%S')} DEBUG (MyTransformOutputOneFileParquet) glue_params:{glue_params}\\\")\\n\\n# 連携元システム名（DMP/ONE/OCTPASS/MANUAL）\\nsource_system = glue_params.get(\\\"SOURCE_SYSTEM\\\", None)\\n# データ名（物理テーブル名）\\ndata_name = glue_params.get(\\\"DATA_NAME\\\", None)\\n# 実行日時\\nexecute_timestamp_iso_str = glue_params.get(\\\"EXECUTE_TIMESTAMP_ISO_STR\\\", None)\\n\\n# プレフィックスヘッド\\nprefix_head = f\\\"{source_system}/{data_name}\\\"\\n\\n# 作業用バケット\\nwork_bucket = \\\"orion-prd-work\\\"\\n# workディレクトリ\\nwork_dir = f\\\"{prefix_head}/raw_work/\\\"\\n# 一時出力先s3パス\\nwork_s3_path = f\\\"s3://{work_bucket}/{work_dir}\\\"\\n\\n# DataFrameの取得\\ndf = dfc.select(list(dfc.keys())[0]).toDF()\\n\\n# workフォルダにparquet形式で出力\\ndf.coalesce(1).write.option(\\\"compression\\\", \\\"snappy\\\").mode(\\\"overwrite\\\").parquet(work_s3_path)\\n\\n# 関数定義\\ndef get_process_timestamp_from_df(df: DataFrame, column_name: str) -> str:\\n    \\\"\\\"\\\"DFからデータ統合基盤処理日時 文字列を取得する\\n    Args:\\n        df (pysoar.sql.DataFrame): データフレーム\\n        column_name (str): データ統合基盤処理日時項目名\\n    Returns:\\n        str: yyyymmddhhmmss\\n    \\\"\\\"\\\"\\n    # JSTタイムスタンプカラムの値を取得（yyyy/mm/dd hh:mm:ss形式）\\n    orion_process_datetm = df.select(column_name).first()[0]\\n    # yyyymmddhhmmss形式に変換\\n    res_datetm_str = datetime.strptime(orion_process_datetm, \\\"%Y/%m/%d %H:%M:%S\\\").strftime(\\\"%Y%m%d%H%M%S\\\")\\n    return res_datetm_str\\n\\ndef convert_timestamp_iso_to_jst(iso_time_str: str, output_format: str) -> str:\\n    \\\"\\\"\\\"ISO日時文字列を指定フォーマットに変換\\n    Args:\\n        iso_time_str (str): ISO日時文字列\\n        output_format (str): 出力フォーマット\\n    Returns:\\n        str: 指定フォーマット形式文字列\\n    \\\"\\\"\\\"\\n    # UTCの日時文字列をパースしてdatetimeオブジェクトに変換\\n    utc_time = datetime.strptime(iso_time_str, \\\"%Y-%m-%dT%H:%M:%S.%fZ\\\")\\n    # JSTへの変換（UTC+9時間）\\n    jst_time = utc_time.replace(tzinfo=timezone.utc).astimezone(timezone(timedelta(hours=9)))\\n    # フォーマット変換\\n    jst_time_str = jst_time.strftime(output_format)\\n    return jst_time_str\\n\\n# ■実行日時をJSTのyyyyymmddhhmmss文字列に変換\\n# DFの処理日時項目名\\nORION_PROCESS_DATETM = \\\"orion_process_datetm\\\"\\nif ORION_PROCESS_DATETM in df.columns and df.select(ORION_PROCESS_DATETM).count() != 0:\\n    execute_timestamp_str = get_process_timestamp_from_df(df=df, column_name=ORION_PROCESS_DATETM)\\n    print(f\\\"{datetime.strftime(datetime.now(), '%Y/%m/%dT%H:%M:%S')} DEBUG (get_process_timestamp_from_df) execute_timestamp_str:{execute_timestamp_str}\\\")\\nelse:\\n    execute_timestamp_str = convert_timestamp_iso_to_jst(iso_time_str=execute_timestamp_iso_str, output_format=\\\"%Y%m%d%H%M%S\\\")\\n    print(f\\\"{datetime.strftime(datetime.now(), '%Y/%m/%dT%H:%M:%S')} DEBUG (convert_timestamp_iso_to_jst) execute_timestamp_str:{execute_timestamp_str}\\\")\\n\\n#  DWHバケット\\ndwh_bucket = \\\"orion-prd-dwh\\\"\\n# 連携先ディレクトリ\\nreceive_prefix = f\\\"{prefix_head}/receive\\\"\\n# 連携ファイル名\\nfile_name = f\\\"{data_name}_dwh_{execute_timestamp_str}.snappy.parquet\\\"\\n# 連携先key\\nfile_key = f\\\"{receive_prefix}/{file_name}\\\"\\n\\n# S3クライアントを使用してworkフォルダに出力したファイルをリネームして移動\\ns3 = boto3.client(\\\"s3\\\")\\ns3_resource = boto3.resource(\\\"s3\\\")\\ntransfer_config = boto3.s3.transfer.TransferConfig(\\n    multipart_threshold=256 * 1024 * 1024,\\n    multipart_chunksize=256 * 1024 * 1024\\n)\\n\\nresponse = s3.list_objects_v2(Bucket=work_bucket, Prefix=work_dir)\\nfor obj in response.get(\\\"Contents\\\", []):\\n    if obj[\\\"Key\\\"].endswith(\\\".parquet\\\"):\\n        # ソース情報設定\\n        copy_source = {\\\"Bucket\\\": work_bucket, \\\"Key\\\": obj[\\\"Key\\\"]}\\n        print(f\\\"{datetime.strftime(datetime.now(), '%Y/%m/%dT%H:%M:%S')} DEBUG (MyTransformOutputOneFileParquet) copy_source:{copy_source}, dwh_bucket:{dwh_bucket}, file_key:{file_key}\\\")\\n        # コピーを実行\\n        s3_resource.meta.client.copy(CopySource=copy_source, Bucket=dwh_bucket, Key=file_key, Config=transfer_config)\\n        # 削除を実行\\n        s3.delete_object(Bucket=work_bucket, Key=obj[\\\"Key\\\"])\\n        break\\n\\nprint(f\\\"{datetime.strftime(datetime.now(), '%Y/%m/%dT%H:%M:%S')} INFO (MyTransformOutputOneFileParquet) end\\\")\\n\",\"Inputs\":[\"node-1731895939959\"],\"Name\":\"【必須】１ファイルでDWHバケットに出力\"}},\"node-1727675945895\":{\"CustomCode\":{\"ClassName\":\"MyTransformEightDateFormat\",\"Code\":\"from pyspark.sql.functions import col, concat, lit, when\\n\\n### 作成時の修正箇所ここから ###\\n# 入力項目名と出力項目名の設定\\n入力項目名リスト = [\\\"入力項目名１\\\", \\\"入力項目名２\\\"]\\n出力項目名リスト = [\\\"出力項目名１\\\", \\\"出力項目名２\\\"]\\n### 作成時の修正箇所ここまで ###\\n\\n# 入力フォーマット\\nINPUT_FORMAT = r\\\"^\\\\d{4}\\\\d{2}\\\\d{2}\\\"\\n\\n# 最小値の設定\\nMIN_YMD = lit(\\\"19000101\\\")\\nMIN_YMD_FORMAT = None\\n\\n# スラッシュをリテラル型で定義\\n# pysparkのwithColunms などでは、文字列は lit()で指定しないと、結果がNullになる\\nLIT_SLASH = lit(\\\"/\\\")\\n\\n# 条件文：\\n#   1)Nullならそのまま\\n#   2)Date文字列で、\\n#       2-1) MIN_YMD より小さいなら MIN_YMD_FORMAT に置換\\n#       2-2)それ以外なら文字列を4・2・2文字ごとに分割し、\\\"/\\\"を間に挿入して結合する\\n#   3)Date文字列でないないら、Noneを返す\\ndef mk_cond(col_name: str):\\n    return (\\n        when(col(col_name).isNull(), None)\\n        .when(col(col_name).substr(1, 8).rlike(INPUT_FORMAT), \\n            when(col(col_name).substr(1, 8) < MIN_YMD, MIN_YMD_FORMAT)\\n            .otherwise(concat(\\n                col(col_name).substr(1, 4), LIT_SLASH, \\n                col(col_name).substr(5, 2), LIT_SLASH, \\n                col(col_name).substr(7, 2)\\n            )))\\n        .otherwise(None)\\n    )\\n\\nconds_dict = {出力項目名: mk_cond(入力項目名) for 入力項目名, 出力項目名 in zip(入力項目名リスト, 出力項目名リスト)}\\n\\n# DataFrameの取得\\ndf = dfc.select(list(dfc.keys())[0]).toDF()\\n\\n# 値のマスキング実施\\ndf = df.withColumns(conds_dict)\\n\\n# DynamicFrameに変換して返す\\noutput_dyf = DynamicFrame.fromDF(df, glueContext, \\\"output\\\")\\nreturn DynamicFrameCollection({\\\"CustomTransform0\\\": output_dyf}, glueContext)\\n\",\"Inputs\":[\"node-1727675890558\"],\"Name\":\"【加工】８桁文字列を日付フォーマットに変換\",\"OutputSchemas\":[{\"Columns\":[{\"Name\":\"work_dt\",\"Type\":\"string\"},{\"Name\":\"odakyu_cust_num\",\"Type\":\"string\"},{\"Name\":\"agt_num\",\"Type\":\"string\"},{\"Name\":\"agt_idtfer_num\",\"Type\":\"string\"},{\"Name\":\"rpstve_cmpy_cd\",\"Type\":\"string\"},{\"Name\":\"cmpy_cd\",\"Type\":\"string\"},{\"Name\":\"office_cd\",\"Type\":\"string\"},{\"Name\":\"store_cd\",\"Type\":\"string\"},{\"Name\":\"trnl_num\",\"Type\":\"string\"},{\"Name\":\"trd_num\",\"Type\":\"string\"},{\"Name\":\"trd_knd\",\"Type\":\"string\"},{\"Name\":\"grt_div\",\"Type\":\"string\"},{\"Name\":\"trd_total_amtmny\",\"Type\":\"decimal(10,2)\"},{\"Name\":\"point_sub_amtmny\",\"Type\":\"decimal(10,2)\"},{\"Name\":\"trd_grt_point\",\"Type\":\"decimal(10,2)\"},{\"Name\":\"trd_gvbk_point\",\"Type\":\"decimal(10,2)\"},{\"Name\":\"trnl_dt\",\"Type\":\"string\"},{\"Name\":\"trnl_tm\",\"Type\":\"string\"},{\"Name\":\"dwh_reg_datetm\",\"Type\":\"timestamp\"},{\"Name\":\"orion_process_datetm\",\"Type\":\"string\"}]}]}},\"node-1731895901595\":{\"CustomCode\":{\"ClassName\":\"MyTransformAddTimestamp\",\"Code\":\"from datetime import datetime, timedelta, timezone\\nfrom awsglue.utils import getResolvedOptions\\nfrom pyspark.sql.functions import lit\\n\\n### 作成時の修正箇所ここから ###\\n#  本ノードは作成時のパラメータ設定不要で、このままお使いください。\\n### 作成時の修正箇所ここまで ###\\n\\n# print(f\\\"{datetime.strftime(datetime.now(), '%Y/%m/%dT%H:%M:%S')} INFO (MyTransformAddTimestamp) start\\\")\\n\\n# Glueジョブ引数の取得\\nglue_params = getResolvedOptions(sys.argv, [\\\"EXECUTE_TIMESTAMP_ISO_STR\\\"])\\n# print(f\\\"{datetime.strftime(datetime.now(), '%Y/%m/%dT%H:%M:%S')} DEBUG (MyTransformAddTimestamp) glue_params:{glue_params}\\\")\\n\\n# 実行日時\\nexecute_timestamp_iso_str = glue_params.get(\\\"EXECUTE_TIMESTAMP_ISO_STR\\\", None)\\n\\n# 追加項目名\\nadd_column_name = \\\"orion_process_datetm\\\"\\n\\n# 関数定義\\ndef convert_timestamp_iso_to_jst(iso_time_str: str, output_format: str) -> str:\\n    \\\"\\\"\\\"ISO日時文字列を指定フォーマットに変換\\n    Args:\\n        iso_time_str (str): ISO日時文字列\\n        output_format (str): 出力フォーマット\\n    Returns:\\n        str: 指定フォーマット形式文字列\\n    \\\"\\\"\\\"\\n    # UTCの日時文字列をパースしてdatetimeオブジェクトに変換\\n    utc_time = datetime.strptime(iso_time_str, \\\"%Y-%m-%dT%H:%M:%S.%fZ\\\")\\n    # JSTへの変換（UTC+9時間）\\n    jst_time = utc_time.replace(tzinfo=timezone.utc).astimezone(timezone(timedelta(hours=9)))\\n    # フォーマット変換\\n    jst_time_str = jst_time.strftime(output_format)\\n    return jst_time_str\\n\\n# ■実行日時をJSTのyyyyy/mm/dd hh:mm:ss文字列に変換\\nexecute_timestamp_str = convert_timestamp_iso_to_jst(iso_time_str=execute_timestamp_iso_str, output_format=\\\"%Y/%m/%d %H:%M:%S\\\")\\n# print(f\\\"{datetime.strftime(datetime.now(), '%Y/%m/%dT%H:%M:%S')} DEBUG (MyTransformAddTimestamp) execute_timestamp_str:{execute_timestamp_str}\\\")\\n\\n# DataFrameの取得\\ndf = dfc.select(*list(dfc.keys())).toDF()\\n\\n# JSTタイムスタンプカラムを追加\\ndf = df.withColumn(add_column_name, lit(execute_timestamp_str))\\n\\n# print(f\\\"{datetime.strftime(datetime.now(), '%Y/%m/%dT%H:%M:%S')} INFO (MyTransformAddTimestamp) end\\\")\\n# DynamicFrameに変換して返す\\noutput_dyf = DynamicFrame.fromDF(df, glueContext, \\\"output\\\")\\nreturn DynamicFrameCollection({\\\"CustomTransform0\\\": output_dyf}, glueContext)\",\"Inputs\":[\"node-1733131877532\"],\"Name\":\"【手動アップロード必須】データ統合基盤処理日時付加\"}},\"node-1733131877532\":{\"SelectFromCollection\":{\"Index\":0,\"Inputs\":[\"node-1733131840543\"],\"Name\":\"【加工時必須】加工後処理(定義した日時フォーマットへ変換)\"}}}",
  "sourceControlDetails" : {
    "provider" : "GITHUB",
    "repository" : "ORioN_nonPIA",
    "branch" : "master",
    "folder" : "main"
  }
}